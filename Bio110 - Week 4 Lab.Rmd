
---
title: "Bio 110 Week 4"
output:
  pdf_document: default
  html_notebook: default
---


# Lab 4 - Using Digital Microscopy to measure Chlamydomonas flagella



The goals of this tutorial are to:
* learn how to import data in a spreadsheet to R Studio
* calculate the mean of your measured flagella lengths
* calculate the 95% confidence intervals of the mean flagella length
* plot the mean as a point with error bars on a graph

Now that you have collected measurements for 10 Chlamydomonas flagella, you must import your spreadsheet of data to R Studio. The format of the columns of data is important, as well as the titles of the columns of data. 

Before you move on and discover how to load your data into R, it might be useful to go over the following checklist that will make it easier to import the data correctly into R:

With spreadsheets, the first row is usually reserved for the header, while the first column is used to identify the sampling unit;
- Avoid names, values or fields with blank spaces, otherwise each word will be interpreted as a separate variable, resulting in errors that are related to the number of elements per line in your data set;
- If you want to concatenate words, inserting a . in between to words instead of a space;
- Short names are prefered over longer names;
- Try to avoid using names that contain symbols such as ?, $,%, ^, &, *, (, ),-,#, ?,,,<,>, /, |, \, [ ,] ,{, and };
- Delete any comments that you have made in your Excel file to avoid extra columns or NAâ€™s to be added to your file
- Make sure that any missing values in your data set are indicated with NA.


Once you have checked that your spreadsheet follows these basic rules, save it as a .csv file in a BIO 110 folder on the desktop.  

First you must upload your .csv file to R Studio in the bottom right window using the Upload button.  Browse to locate your file and click Upload.  Make sure it's loaded into the bio110-week3 folder.

Before writing any R code, we'll need to load some R packages in order to use some of the specific functions we want.  Run the code chunk below to make sure these functions are available to your environement:

```{r}

# the "tidyverse" loads multiple packages that we need, like ggplot, dplyr, and readr

library(tidyverse)



```
 

Just like last week, you'll want to load your data as a data frame into R by assigning it to a variable. It's important that your csv file is structured like this:

time,length
25,1.9
25,4.4
25,3.6
25,0.6
....
85,7.1
85,6.8



Upload your file to the Files section to the right. Then load your file to a variable using the read_csv() function, and print your variable to confirm:

```{r}
# for example,copy and paste the following command below, using your .csv file name in "":
# myData<-read_csv("myData.csv")



# confirm by printing your variable



```


Our end goal is to plot the mean of each measurement with 95% confidence interval bars, grouped by each time interval (e.g. all the measurements at 25, 40, 55, etc.). As such we're going to use some new coding strategies to get our data in the form we want. 

One common approach to this is to use the "group_by" function in tandem with the "summarise" function. Furthermore, we're going to use the R "pipe" operator %>%, which is used to pass data to new functions. Here's an example command:

myStats<-myData %>% group_by(time) %>% summarise (Average=mean(length), StandardDeviation=sd(length), n=n())

Let's analyze what's happening here to create the new data frame myStats

1) I'm starting with my loaded data (myData)

2) I'm using the %>% command to pass myData to the group_by function, and indicating that the data should be grouped by the "time" column header.

3) Then, I'm "piping" this to the "summarise" function, which lets me create columns in my new data frame "myStats" based upon formulas. In this case, I'm creating a column called "Average" and setting it equal to the mean of the grouped "length" column from myData. I'm also creating the column "StandardDeviation", and setting it equal to the standard deviation of the grouped data, using the sd() function. Finally I'm creating the column "n", which uses the n() function to count the number of measurements in each group.

So the new "myStats" data frame should have a structure that looks like this (the #'s are just placeholders for real computed values):

time, Average, StandardDeviation, n
25, #, #, #
40, #, #, #
55, #, #, #
70, #, #, #
85, #, #, #

This new data frame structured this way is the precursor for creating the plot that we want that shows the differences in groups.

Go ahead and try the group_by/summarise technique with your data, using the code listed above.

```{r}
# create a new data frame variable, adding columns for mean, standard deviation, and number of measurements 


# print the variable to check your success


```


The grouped data frame we just created is pretty good, but we need to add the upper and lower limits of the 95% confidence interval for our plot error bars. Here's the formula for computing upper- and lower- confidence interval values:

upperCI<-myMean+((qnorm(0.975)*myStdDev)/sqrt(n))

lowerCI<-myMean-((qnorm(0.975)*myStdDev)/sqrt(n))

In the formulas above, myMean, myStdDev, and n are all variables that could be calculated in the "summarise" function used earlier. In the code block below, try updating your group_by/summarise code from earlier to add lowerCI and upperCI:


```{r}

# create a new data frame variable, adding columns for mean, standard deviation, number of measurements, upperCI, and lowerCI


# print the variable to check your success





```




Now with the data in this format, we should be able to create a visualization. Here are the charactaristics we want:

* a point plotted for each group
* Time on the x-axis
* Average on the y-axis
* Error bars for each point, showing the 95% confidence interval
* A line through each point

To see a rough example, click on the "sample.png" file in the Files window.

The best way to approach this is to try a little bit at a time, test the output, add a little more, and test again, etc.

To start, try just generating a plot with points. You'll want to use ggplot, with this general format:

****** How much prompting should they have? *******
ggplot(data=fullData, mapping=aes(x=time, y=avg))+geom_point()


```{r}

# Generate a point plot of your data below, with time on the x-axis, and average on the y-axis, using geom_point()




```


Now add the error bars, using geom_error(). Your ymin and ymax values should be your calculated lower and upper 95% confidence interval values, respectively.

```{r}

# Generate your plot with geom_point(), and add error bars using geom_errorbar() 





```


next - adjust x/y axes


next - add line




/********TESTING!!***************/

```{r}

data<-read_csv("chlamydataforR.csv")
data


myStats<-data %>% group_by(time) %>% dplyr::summarise (avg=mean(length), sd=sd(length))

myStats



compute_lower_ci <- function(mean, sd, n, conf_level = 0.95){
  error <- qnorm(0.975)*sd/sqrt(n)
  lower_ci<-mean - error
}


compute_upper_ci <- function(mean, sd, n, conf_level = 0.95){
  error <- qnorm(0.975)*sd/sqrt(n)
  upper_ci<-mean+error
}


#### NOTE!! If using the Rmisc library, may need to specify the dplyr library when calling summarise
## e.g. dplyr::summarise(....)


fullData<-group_by(.data=data, time) %>% dplyr::summarize(avg=mean(length), stdDev=sd(length), n=n(), stdEr=sd(length)/sqrt(n())) %>% mutate(se = stdDev / sqrt(n),
         lower_ci = compute_lower_ci(avg, stdDev, n),
         upper_ci = compute_upper_ci(avg, stdDev, n))

fd<-data %>% group_by(time) %>% dplyr::summarize(
  avg=mean(length), 
  stdDev=sd(length), 
  n=n(), 
  upper_ci = avg+((qnorm(0.975)*stdDev)/sqrt(n)), 
  lower_ci = avg-((qnorm(0.975)*stdDev)/sqrt(n))
  )


fullData

fd



```





## plotting

```{r}


ggplot(data=fullData, mapping=aes(x=time, y=avg))+ geom_path(mapping=aes(x=time, y=avg))+geom_point()+ylim(0,10)+xlim(0,90)+geom_errorbar(mapping=aes(ymin=lower_ci, ymax=upper_ci), width=5)


ggplot(data=fullData, mapping=aes(x=time, y=avg))+geom_point()

## need to extend line to 0,0, clean up, etc.


```








Testing with summarySE to make sure the values match



```{r}


library(Rmisc)



twentyFive<-read_csv("25.csv")


myStats<-summarySE(data=twentyFive, measurevar="length", conf.interval = 0.95)

myStats%>% mutate(lowerci=length-ci, upperci=length+ci) 


qnorm(0.975)


```





## Generating statistics for plots

Our goal is to generate a plot showing the mean of our data as a point, with error bars displaying a 95% confidence interval. A 95% confidence interval defines a range of values that you can be 95% certain contains the population mean. 

There's a function in R called "summarySE" that can generate the data we need for this plot. Here's the basic structure:

myStats<-summarySE(data=myData, measurevar="MyColumnName", conf.interval = 0.95)

where myData is your data frame, "MyColumnName" is the name of the column to measure, and conf.interval is the percent confidence interval you want to compute. 

Adapt the summarySE command to your data, and run and print it below:

```{r}
# assign a variable to the summarySE function as described above:


# print the variable:

```



This function generates a new data frame with the calculated statistics mean (the column name will match "MyColumnName"), standard deviation (sd), standard error (se), and confidence interval (ci). In order to get and use this data, you'll need to create new variables. You can do this by using the "$" operator, which lets you specify a specific column within a dataframe. For example, if your data frame is named myStats, and you want to get the "ci" data, your variable would look like this:

myCI<-myStats$ci

Below, create variables for the mean, ci, and the upper and lower limits of the confidence interval (hint: the upper limit is the mean+ci, lower is mean-ci)


```{r}
# create variable for the mean

# create variable for the ci value

# create variable for ci upper limit

# create variable for ci lower limit


```

## Plots with ggplot

Now we have the data needed to create a plot. Beginning with this lab, we're going to be using "ggplot", which is a very popular library for creating data visualizations in R. We're likely going to use ggplot for the remainder of the labs this semester.

The "gg" translates to "grammar of graphics", and is founded in the idea that all data visualizations are comprised of three components:

* data set
* aesthetics, or visual marks that represent the data (i.e. the stuff that you see)
* geometric objects, or "geoms" (e.g the type of plot)

Let's first look at the basic structure of ggplot code:

ggplot(data=myData, mapping=aes(x=someValue, y=anotherValue))+geom_point()

In the code above, the three components are represented as such:

* data set is "myData", typically a variable representing a data frame
* aesthetics (mapping=aes(...)) refers to the points or data that appear. The values in the aes portion can vary depending upon the type of plot you create (refer to the ggplot2-cheatsheet.pdf in the files section for more info). x and y could be a single point or variable, refer to a column of data, or could be text.
* the geom, or type of plot, is represented by "geom_point()". Each type of plot has a different function (e.g. geom_bar(), geom_line(), etc.).

Notice that "geom_point()" is proceeded with a "+". The "+" is part of the ggplot syntax, and is a way to "chain" functions together. We'll do more of this soon to add more components to the plot. 

Start off by creating a plot using the code sample above. Make sure you use your variable for the data frame in the "data=" portion, use your mean variable for the y-value, and set the x-value equal to some descriptive text (since it's text, make sure you use quotes).

```{r}

# ggplot code here:



```

You'll notice that the y-axis has an auto-generated range. Let's explicitly define our range by chaining the "ylim" function to the end of our plot. ylim takes an ordered pair, representing the minimum and maximim y-axis values (e.g. ylim(0,100)). Make sure you start with 0, and use a reasonable value for the max.

```{r}
# redo your plot, and chain "ylim" to the end


```

Now let's add the error bars representing the 95% confidence interval. It's the same code you just wrote, plus chaining the geom_errorbar() function. Since the error bar is "stuff that you see", you use the same "mapping=aes(...)" syntax as in the ggplot() function. In this case, it would look like:

geom_errorbar(mapping=aes(ymin=minValue, ymax=maxValue))

You can also adjust the width of your errorbar like this:

geom_errorbar(mapping=aes(ymin=minValue, ymax=maxValue), width=0.5)

Below, chain this function on to your plot code, and use your confidence interval min & max for the ymin and ymax values. Play around with different values for "width", until it seems ok to you:

```{r}

# plot code, adding the geom_errorbar() function


```


Finally, you can add custom labels by chaining the labs() function to your ggplot code. The syntax is:

labs(title="my title", x="my x label", y="my y axis label")

Chain this to your code, and run it below:

```{r}

# final plot, chaining the labs() function:


```



Make sure you copy and save your plot and print it with a detailed caption to turn in to your instructor.
